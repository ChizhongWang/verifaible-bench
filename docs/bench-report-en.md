# VerifAIble Bench: Evaluating LLM Agents on Verifiable Evidence Collection from Real-World Web Sources

> **VerifAIble Bench Evaluation Report**
>
> Published: February 2026

---

## Abstract

Large Language Models (LLMs) excel at generative tasks, yet their tendency to "hallucinate" severely limits adoption in high-stakes domains. We introduce **VerifAIble Bench**‚Äîthe first end-to-end benchmark for evaluating LLM Agents on *verifiable evidence collection*. The benchmark comprises **21 test cases** spanning static text, HTML tables, JavaScript-rendered dynamic pages, PDF documents, and video transcripts, drawn from financial regulatory portals in China and the United States. We evaluate 4 models (GLM-5, Kimi-K2.5, MiniMax-M2.5, DeepSeek-R1) using an all-or-nothing gated scoring system across four dimensions: answer correctness, citation creation, citation embedding, and evidence type matching. Results show that **GLM-5 achieves a perfect score of 2,100**, Kimi-K2.5 and MiniMax-M2.5 tie at 1,680 (average 80.0), and DeepSeek-R1 trails at 1,080 (average 51.4). Dynamic page interaction emerges as the dominant differentiator across models. Cost analysis based on OpenRouter API pricing reveals that GLM-5 not only scores highest but also costs the least ($0.64 total), achieving a cost-efficiency (score/dollar) **3.4√ó higher** than the lowest-ranked model. All models complete the full 21-case benchmark for under $1.20, validating the economic feasibility of agent-based evidence collection at scale.

---

## 1. Introduction

### 1.1 LLM Hallucination and the Need for Grounding

Large Language Models have achieved remarkable progress in open-domain question answering and content generation, but their inherent "hallucination" problem‚Äîgenerating plausible yet factually incorrect content‚Äîseverely constrains their use in finance, law, journalism, and other domains requiring high verifiability. To address this, the industry has widely adopted Grounding strategies, requiring model outputs to be traceable to verifiable original sources.

### 1.2 Limitations of Existing Benchmarks

Existing RAG (Retrieval-Augmented Generation) benchmarks primarily evaluate retrieval quality and answer generation quality, but have significant blind spots:

1. **No evidence verifiability dimension**: They only assess whether answers are correct, not whether answers can be independently verified by third parties
2. **Ignore real-world web interaction**: Most benchmarks use pre-processed static corpora without addressing JavaScript dynamic rendering, date filtering, pagination, and other real-world web operations
3. **No end-to-end evidence pipeline evaluation**: The complete pipeline from search ‚Üí navigation ‚Üí interaction ‚Üí collection ‚Üí citation lacks systematic assessment

### 1.3 VerifAIble Platform and VerifAIble Bench

**VerifAIble** is a verifiable evidence management platform that supports highlight annotation and playback verification of web evidence. Building on this platform, we designed **VerifAIble Bench** to evaluate the ability of LLM Agents to complete the full pipeline of "search ‚Üí page interaction ‚Üí data collection ‚Üí citation creation."

Unlike traditional RAG benchmarks, VerifAIble Bench requires models not only to answer correctly, but also to:
- Create verifiable citations pointing to original web sources
- Embed citation markers within the answer text
- Collect the correct type of evidence (text/table/PDF/video)

---

## 2. Benchmark Design

### 2.1 Test Set Composition

VerifAIble Bench contains **21 test cases** distributed across five categories:

| Category | Cases | Description | Difficulty |
|----------|-------|-------------|------------|
| Static Text | 6 | Extract data directly from web page body text | ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ |
| Static Table | 7 | Locate specific row-column data from HTML tables | ‚òÖ‚òÖ‚òÜ‚òÜ‚òÜ |
| Dynamic Page | 4 | Execute JS operations (date selection, pagination) before collecting table data | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ |
| Dynamic+PDF | 2 | Search and open PDF documents within dynamic pages to extract information | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ |
| Video | 2 | Extract specific information from video transcripts | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ |

**Data Source Distribution:**

| Source | Cases | Domain |
|--------|-------|--------|
| National Bureau of Statistics (China) | 8 | stats.gov.cn |
| Shanghai Stock Exchange | 6 | sse.com.cn |
| China Securities Regulatory Commission | 1 | csrc.gov.cn |
| Federal Reserve | 3 | federalreserve.gov |
| U.S. Treasury | 1 | treasury.gov |
| YouTube | 2 | youtube.com |

### 2.2 Agent Tool Set

The LLM Agent is equipped with **6 tools** during evaluation:

| Tool | Function |
|------|----------|
| `verifaible_web_search` | Search the internet for relevant pages |
| `web_fetch` | Retrieve web page content from a specified URL |
| `analyze_page` | Analyze page structure, identify key elements and interaction methods |
| `test_action_steps` | Execute action steps on pages (click, input, JS execution), equivalent to browser automation |
| `verifaible_cite` | Create verifiable citations and save evidence snapshots |
| `video_transcript` | Retrieve YouTube video transcript content |

The `test_action_steps` tool supports `exec_js` capabilities, enabling execution of arbitrary JavaScript code (e.g., setting date pickers, triggering AJAX requests, manipulating DOM elements)‚Äîa critical tool for completing dynamic page tasks.

### 2.3 Scoring Criteria

VerifAIble Bench employs an **All-or-Nothing Gated Scoring System**:

**Gate Condition:** If the answer is not completely correct (answerCorrect ‚â† 1.0), the total score for that case is **0**.

**Four-Dimensional Scoring (Maximum 100 points):**

| Dimension | Weight | Description |
|-----------|--------|-------------|
| Answer Correctness (answerCorrect) | 40 | Whether the model answer contains all expected keywords |
| Citation Created (citationCreated) | 25 | Whether `verifaible_cite` was called to create a citation |
| Citation In Text (citationInText) | 15 | Whether the answer text contains `[@v:ID]` citation markers |
| Evidence Type Match (evidenceTypeMatch) | 20 | Whether the cited evidence type matches the expected type |

**Scoring Formula:**

```
if answerCorrect < 1.0:
    totalScore = 0                          # Gate: wrong answer ‚Üí zero
else:
    totalScore = 40
              + citationCreated √ó 25
              + citationInText  √ó 15
              + evidenceTypeMatch √ó 20
```

This results in only three possible scores per case: **100** (perfect), **80** (correct answer but evidence type mismatch), or **0** (incorrect answer).

### 2.4 Experimental Configuration

| Setting | Value |
|---------|-------|
| API | OpenRouter Responses API |
| Temperature | 0.3 |
| Max Round Trips | 30 |
| Session Isolation | Independent session per case, no shared state |

---

## 3. Evaluated Models

Four LLMs accessible via OpenRouter were selected for this evaluation:

| Model | Provider | Characteristics |
|-------|----------|----------------|
| **GLM-5** | Zhipu AI (Z-AI) | Next-generation general-purpose model with strong tool-calling capabilities |
| **Kimi-K2.5** | Moonshot AI | Excels at long-context understanding with multimodal support |
| **MiniMax-M2.5** | MiniMax | Cost-effective model with large context window |
| **DeepSeek-R1** | DeepSeek | Reasoning-specialized model with deep Chain-of-Thought inference |

---

## 4. Results

### 4.1 Overall Rankings

| Rank | Model | Total | Avg | Pass | Full | Avg Rounds | Avg Tool Calls | Total Time | API Cost |
|------|-------|-------|-----|------|------|-----------|---------------|------------|----------|
| ü•á | **GLM-5** | **2,100** | **100.0** | **21/21** | **21/21** | 7.7 | 6.7 | 55.7 min | **$0.64** |
| ü•à | **MiniMax-M2.5** | 1,680 | 80.0 | 17/21 | 16/21 | 10.6 | 9.7 | 71.3 min | $1.10 |
| ü•à | **Kimi-K2.5** | 1,680 | 80.0 | 17/21 | 16/21 | 8.0 | 7.0 | 60.6 min | $0.73 |
| 4 | DeepSeek-R1 | 1,080 | 51.4 | 11/21 | 10/21 | 5.0 | 4.0 | 115.0 min | $1.11 |

> **Note:** Pass = cases with score > 0; Full = cases with score = 100.

### 4.2 Category Analysis

| Category | Cases | GLM-5 | MiniMax-M2.5 | Kimi-K2.5 | DeepSeek-R1 |
|----------|-------|-------|-------------|-----------|-------------|
| Static Text | 6 | **600** (100%) | 580 (96.7%) | **600** (100%) | **600** (100%) |
| Static Table | 7 | **700** (100%) | **700** (100%) | 600 (85.7%) | 580 (82.9%) |
| Dynamic Page | 4 | **400** (100%) | 0 (0%) | 100 (25%) | 0 (0%) |
| Dynamic+PDF | 2 | **200** (100%) | **200** (100%) | 180 (90%) | 0 (0%) |
| Video | 2 | **200** (100%) | **200** (100%) | **200** (100%) | 0 (0%) |

**Key Findings:**

- **Static content (Text + Table)**: All four models perform similarly well
- **Dynamic pages are the dominant differentiator**: Only GLM-5 completed all 4 dynamic cases; other models scored 0%‚Äì25%
- **Video comprehension**: All models except DeepSeek-R1 successfully utilized the `video_transcript` tool
- **DeepSeek-R1 fails across all non-static categories**: Zero scores on all 8 dynamic, PDF, and video cases

### 4.3 Token Usage and Efficiency Analysis

| Model | Input Tokens | Output Tokens | Total Tokens | Output/Input Ratio |
|-------|-------------|---------------|-------------|-------------------|
| GLM-5 | 1.87M | 31.5K | 1.90M | 1.7% |
| MiniMax-M2.5 | 3.45M | 54.4K | 3.51M | 1.6% |
| Kimi-K2.5 | 2.65M | 39.8K | 2.70M | 1.5% |
| DeepSeek-R1 | 1.02M | 160.2K | 1.18M | 15.7% |

**Efficiency Metrics:**

| Model | Score | Total Tokens | Total Time | API Cost | Score/10K Tokens | Score/min | Score/$ |
|-------|-------|-------------|------------|----------|-----------------|----------|---------|
| **GLM-5** | 2,100 | 1.90M | 55.7 min | $0.64 | **11.1** | **37.7** | **3,281** |
| MiniMax-M2.5 | 1,680 | 3.51M | 71.3 min | $1.10 | 4.8 | 23.6 | 1,527 |
| Kimi-K2.5 | 1,680 | 2.70M | 60.6 min | $0.73 | 6.2 | 27.7 | 2,301 |
| DeepSeek-R1 | 1,080 | 1.18M | 115.0 min | $1.11 | 9.2 | 9.4 | 971 |

> GLM-5 leads significantly across all three efficiency dimensions: score-per-token, score-per-minute, and score-per-dollar. API costs computed using OpenRouter pricing as of February 2026.

### 4.4 Conversational Behavior Analysis

| Model | Avg Rounds | Avg Tool Calls | Avg Input Tokens | Avg Output Tokens | Avg Duration |
|-------|-----------|---------------|-----------------|------------------|-------------|
| GLM-5 | 7.7 | 6.7 | 88.9K | 1.5K | 2m39s |
| MiniMax-M2.5 | 10.6 | 9.7 | 164.4K | 2.6K | 3m24s |
| Kimi-K2.5 | 8.0 | 7.0 | 126.4K | 1.9K | 2m53s |
| DeepSeek-R1 | 5.0 | 4.0 | 48.4K | 7.6K | 5m29s |

**The DeepSeek-R1 CoT Paradox:**
- Fewest round trips (5.0) and tool calls (4.0) among all models
- Yet **3‚Äì5√ó more output tokens** per case (7.6K vs 1.5‚Äì2.6K)
- Longest duration (5m29s/case) and lowest score (51.4)

This demonstrates that R1's Chain-of-Thought reasoning generates extensive "thinking" text that fails to translate into better tool-use strategies. Its fewer tool calls mean it cannot complete tasks requiring multi-step interaction.

### 4.5 API Cost Analysis

Using OpenRouter pricing as of February 2026, we computed the total API cost for each model to complete all 21 test cases.

**OpenRouter Model Pricing:**

| Model | Input Price ($/M tokens) | Output Price ($/M tokens) |
|-------|-------------------------|--------------------------|
| GLM-5 | $0.30 | $2.55 |
| Kimi-K2.5 | $0.23 | $3.00 |
| MiniMax-M2.5 | $0.30 | $1.20 |
| DeepSeek-R1 | $0.70 | $2.50 |

**Cost Breakdown by Model:**

| Model | Input Tokens | Output Tokens | Input Cost | Output Cost | **Total Cost** | Cost/Case |
|-------|-------------|---------------|-----------|------------|--------------|-----------|
| **GLM-5** | 1.87M | 31.5K | $0.56 | $0.08 | **$0.64** | $0.031 |
| Kimi-K2.5 | 2.65M | 39.8K | $0.61 | $0.12 | **$0.73** | $0.035 |
| MiniMax-M2.5 | 3.45M | 54.4K | $1.04 | $0.07 | **$1.10** | $0.052 |
| DeepSeek-R1 | 1.02M | 160.2K | $0.71 | $0.40 | **$1.11** | $0.053 |

**Cost-Efficiency Rankings:**

| Rank | Model | Score | Total Cost | Score/$ | Cost/Point |
|------|-------|-------|-----------|---------|-----------|
| ü•á | **GLM-5** | 2,100 | $0.64 | **3,281** | $0.0003 |
| ü•à | Kimi-K2.5 | 1,680 | $0.73 | 2,301 | $0.0004 |
| ü•â | MiniMax-M2.5 | 1,680 | $1.10 | 1,527 | $0.0007 |
| 4 | DeepSeek-R1 | 1,080 | $1.11 | 971 | $0.0010 |

**Key Findings:**

- **GLM-5 achieves the lowest cost and highest cost-efficiency**: It completes all 21 tests for just $0.64 with a perfect score, delivering a score-per-dollar ratio **3.4√ó higher** than DeepSeek-R1
- **Input tokens dominate costs**: For GLM-5, Kimi-K2.5, and MiniMax-M2.5, input costs account for **84%‚Äì94%** of total spend, meaning reducing conversation rounds (and thus context accumulation) is the most effective cost optimization strategy
- **DeepSeek-R1's output cost anomaly**: Despite consuming the fewest input tokens (1.02M), its extensive CoT reasoning text pushes output costs to **36%** of total spend‚Äîfar above the 6%‚Äì16% range of other models
- **Lower unit price ‚â† lower total cost**: MiniMax-M2.5 has the cheapest output rate ($1.20/M), yet its heavy input token consumption (3.45M) results in the third-highest total cost
- **All models cost under $1.20 total**: For a 21-case benchmark, agent-based evidence collection remains economically viable at scale across all evaluated models

---

## 5. In-Depth Analysis

### 5.1 Why Did GLM-5 Achieve a Perfect Score?

GLM-5 scored 100 on all 21 test cases. Key success factors include:

1. **Efficient dynamic page interaction strategy**: When facing SSE's date pickers, GLM-5 accurately identified DOM structures and correctly set dates via `exec_js`, typically completing the most complex dynamic tasks within 10‚Äì28 rounds
2. **Precise tool-call decisions**: An average of only 6.7 tool calls per case‚Äîthe "just right" amount
3. **Consistent citation creation**: 100% citation creation rate, correctly calling `verifaible_cite` and embedding citation markers every time

### 5.2 Dynamic Pages as the Primary Discriminator

The 4 dynamic page cases from the Shanghai Stock Exchange (sse.com.cn) proved to be the most discriminating tests in the entire benchmark:

| Case | Task | GLM-5 | MiniMax | Kimi | R1 |
|------|------|-------|---------|------|-----|
| cn_dynamic_001 | Bond transaction count (date filter) | ‚úÖ 100 | ‚ùå 0 | ‚úÖ 100 | ‚ùå 0 |
| cn_dynamic_002 | Top market cap stock ratio (date filter) | ‚úÖ 100 | ‚ùå 0 | ‚ùå 0 | ‚ùå 0 |
| cn_dynamic_003 | Fund trading volume (date filter) | ‚úÖ 100 | ‚ùå 0 | ‚ùå 0 | ‚ùå 0 |
| cn_dynamic_004 | Securities lending balance (date filter) | ‚úÖ 100 | ‚ùå 0 | ‚ùå 0 | ‚ùå 0 |

**Failure Analysis:**
- **Date setting failure**: MiniMax and Kimi retrieved data for the wrong date in cn_dynamic_002‚Äì004 (e.g., confusing 2025-02-10 with 2026-02-10), indicating they failed to correctly operate the page's date filter components
- **Round trip exhaustion**: MiniMax exhausted the 30-round limit in cn_dynamic_001 and cn_dynamic_003 without completing the task
- **Premature termination**: DeepSeek-R1 typically stopped tool calls after 5‚Äì10 rounds, even without obtaining the correct answer

### 5.3 The Reasoning Model Paradox of DeepSeek-R1

DeepSeek-R1, known for its Chain-of-Thought (CoT) reasoning capabilities, performed worst in this benchmark, exhibiting a clear "reasoning-action" paradox:

| Metric | DeepSeek-R1 | Other Models (avg) | Ratio |
|--------|-----------|-------------------|-------|
| Avg Output Tokens | 7,630 | 2,000 | **3.8√ó** |
| Avg Rounds | 5.0 | 8.8 | **0.57√ó** |
| Avg Tool Calls | 4.0 | 7.8 | **0.51√ó** |
| Score | 51.4 | 86.7 | **0.59√ó** |
| Total Time | 115.0 min | 62.5 min | **1.84√ó** |

R1 allocates substantial computational resources to generating CoT reasoning text, but these reasoning traces fail to translate into effective tool-use behavior:
- In cn_dynamic_003, R1 produced 31,201 output tokens in just 5 rounds (6,240 tokens/round) yet achieved zero successful page interactions
- In cn_dynamic_pdf_001 and cn_dynamic_pdf_002, R1 spent nearly 19 minutes each but failed to locate the target PDFs

**Conclusion**: For Agent tasks requiring "frequent tool calls + environment interaction," improvements in pure reasoning capability cannot compensate for deficiencies in action capability.

---

## 6. Limitations and Future Work

### 6.1 Current Limitations

1. **Limited test set size**: Only 21 cases, insufficient to cover all possible evidence collection scenarios
2. **Limited model selection**: Only 4 models evaluated; GPT-4o, Claude, and other mainstream models are not included
3. **Concentrated data sources**: Primarily focused on Chinese and U.S. financial regulatory websites, lacking coverage of other domains
4. **Single run**: No repeated experiments to assess the stability of model performance
5. **Pricing volatility**: API costs are based on an OpenRouter pricing snapshot from February 2026; actual rates may change over time

### 6.2 Future Work

1. **Expand test set**: Increase to 100+ cases covering additional domains (healthcare, law, academia)
2. **Include more models**: Add GPT-4o, Claude, Gemini, and other international models for cross-comparison
3. **Multilingual expansion**: Add test scenarios in Japanese, Korean, and other Asian languages
4. **Stability assessment**: Run each case multiple times to evaluate performance variance
5. **Dynamic cost tracking**: Continuously update API pricing data to track long-term trends in cost-efficiency across models

---

## Appendix

### Appendix A: Complete Score Matrix

| # | Case ID | Category | Question Summary | GLM-5 | MiniMax | Kimi | R1 |
|---|---------|----------|-----------------|-------|---------|------|----|
| 1 | cn_text_001 | text | 2025 China total population | 100 | 100 | 100 | 100 |
| 2 | cn_text_002 | text | CSRC securities regulatory bureaus count | 100 | 100 | 100 | 100 |
| 3 | cn_text_table_001 | text/table | Jan 2026 PMI | 100 | 100 | 100 | 100 |
| 4 | cn_table_001 | table | Rebar price change (Jan late) | 100 | 100 | 100 | 100 |
| 5 | cn_table_002 | table | Zhengzhou used housing price YoY | 100 | 100 | **0** | **0** |
| 6 | cn_table_003 | table | Rental housing rent YoY | 100 | 100 | 100 | 100 |
| 7 | cn_table_004 | table | Durable goods price YoY | 100 | 100 | 100 | 80 |
| 8 | cn_table_005 | table | Non-manufacturing business activity index | 100 | 100 | 100 | 100 |
| 9 | cn_table_006 | table | Rebar price change (Feb early) | 100 | 100 | 100 | 100 |
| 10 | cn_dynamic_001 | dynamic | Gov bond transaction count | 100 | **0** | 100 | **0** |
| 11 | cn_dynamic_002 | dynamic | Top market cap stock ratio | 100 | **0** | **0** | **0** |
| 12 | cn_dynamic_003 | dynamic | Fund trading volume | 100 | **0** | **0** | **0** |
| 13 | cn_dynamic_004 | dynamic | Securities lending balance | 100 | **0** | **0** | **0** |
| 14 | cn_dynamic_pdf_001 | dynamic+pdf | China Gold Zhengzhou branch head | 100 | 100 | 80 | **0** |
| 15 | cn_dynamic_pdf_002 | dynamic+pdf | SPDB net asset per share change | 100 | 100 | 100 | **0** |
| 16 | us_text_001 | text | IORB rate (Feb 2024) | 100 | 100 | 100 | 100 |
| 17 | us_text_002 | text | FOMC dissenting vote | 100 | 100 | 100 | 100 |
| 18 | us_text_003 | text | IORB rate cut to | 100 | 80 | 100 | 100 |
| 19 | us_table_001 | table | 10-year Treasury yield | 100 | 100 | 100 | 100 |
| 20 | video_001 | video | Ken Robinson TED talk quote | 100 | 100 | 100 | **0** |
| 21 | video_002 | video | Neural network input layer neurons | 100 | 100 | 100 | **0** |
| | | | **Total** | **2,100** | **1,680** | **1,680** | **1,080** |

### Appendix B: Failure Case Error Analysis

| Case | Model | Failure Reason |
|------|-------|---------------|
| cn_table_002 | Kimi | Wrong answer: failed to accurately locate Zhengzhou used housing YoY data from complex table |
| cn_table_002 | R1 | Wrong answer: returned 91.1 (correct: 90.9), likely read adjacent row data |
| cn_table_004 | R1 | Evidence type mismatch: extracted from body text instead of table (scored 80) |
| cn_dynamic_001 | MiniMax | Round trip exhaustion: failed to complete date setting within 30 rounds |
| cn_dynamic_001 | R1 | Wrong date: retrieved wrong date's data (167 vs correct 61) |
| cn_dynamic_002 | MiniMax, Kimi | Date confusion: returned 5.33% (2026 data) instead of 3.97% (2025 data) |
| cn_dynamic_002 | R1 | Date confusion: failed to correctly switch to Feb 2025 data |
| cn_dynamic_003 | MiniMax | Round trip exhaustion: 30 rounds without completion |
| cn_dynamic_003 | Kimi | Round trip exhaustion: 30 rounds without completion |
| cn_dynamic_003 | R1 | Premature stop: stopped after 5 rounds, generated extensive CoT text but no effective actions |
| cn_dynamic_004 | MiniMax, Kimi, R1 | Date confusion / partial match: all retrieved wrong date's lending data |
| cn_dynamic_pdf_001 | R1 | Navigation failure: could not find target PDF in announcements page |
| cn_dynamic_pdf_001 | Kimi | Evidence type mismatch: correct answer but cited as text instead of pdf (scored 80) |
| cn_dynamic_pdf_002 | R1 | Navigation failure: could not find target PDF in announcements page |
| us_text_003 | MiniMax | Evidence type mismatch: cited as pdf instead of text (scored 80) |
| video_001 | R1 | Answer matching failure: included correct quote but scoring engine failed to match full phrase |
| video_002 | R1 | No citation created: answered the question but did not call `verifaible_cite` |

### Appendix C: Token Usage Details

| Model | Total Input Tokens | Total Output Tokens | Total Tokens | Avg Input/Case | Avg Output/Case |
|-------|-------------------|--------------------|--------------| --------------|----------------|
| GLM-5 | 1,866,536 | 31,507 | 1,898,043 | 88,883 | 1,500 |
| MiniMax-M2.5 | 3,453,171 | 54,435 | 3,507,606 | 164,437 | 2,592 |
| Kimi-K2.5 | 2,654,870 | 39,759 | 2,694,629 | 126,422 | 1,893 |
| DeepSeek-R1 | 1,016,392 | 160,241 | 1,176,633 | 48,400 | 7,631 |

---

*The complete dataset and code are open-sourced in the [verifaible-bench](https://github.com/verifaible/verifaible-bench) repository.*
